[{"title":"Brain-Controlled Robotic Arm System Based on Multi-Directional CNN-BiLSTMNetwork Using EEG Signals","url":"/2021/06/30/Brain-Controlled-Robotic-Arm-System-Based-on-Multi-Directional-CNN-BiLSTM-Network-Using-EEG-Signals/"},{"title":"DSNet: A Flexible Detect-to-Summarize Network for Video Summarization","url":"/2021/05/24/DSNet/","content":"\n### 0. Overview\n\n&emsp;&emsp;&emsp;&emsp;代码：https://github.com/li-plus/DSNet\n&emsp;&emsp;&emsp;&emsp;兴趣程度 6\n\n### 1. Details\n\n![](/images/DSNet/2.png)\n\n![](/images/DSNet/3.png)\n\n![](/images/DSNet/6.png)\n\n![](/images/DSNet/7.png)\n\n![](/images/DSNet/8.png)\n\n![](/images/DSNet/9.png)\n\n![](/images/DSNet/10.png)\n\n![](/images/DSNet/11.png)\n\n![](/images/DSNet/12.png)\n\n![](/images/DSNet/13.png)\n\n![](/images/DSNet/14.png)\n\n![](/images/DSNet/15.png)\n\n![](/images/DSNet/16.png)\n\n![](/images/DSNet/17.png)\n\n![](/images/DSNet/18.png)\n\n![](/images/DSNet/19.png)\n\n![](/images/DSNet/20.png)\n\n![](/images/DSNet/21.png)\n\n![](/images/DSNet/22.png)\n\n![](/images/DSNet/23.png)\n\n![](/images/DSNet/24.png)\n\n![](/images/DSNet/25.png)\n\n![](/images/DSNet/26.png)\n\n","tags":["LSTM"],"categories":["Paper Reading"]},{"title":"HSA-RNN: Hierarchical Structure-Adaptive RNN for Video Summarization","url":"/2021/04/20/HSA-RNN-Hierarchical-Structure-Adaptive-RNN-for-Video-Summarization/","content":"\n### 0. Overview\n\n![](/HSA-RNN-Hierarchical-Structure-Adaptive-RNN-for-Video-Summarization/1.png)\n\n优点：frame-shot-video分层结构，双向LSTM滑动窗口\n缺点：纯pixel features，supervised，importance score不新颖，滑动窗口k固定\n兴趣程度：6\n\n\n\n### 1. Understanding\n\n中规中矩的一篇文章。提出了frame-shot-video的层级结构。先从frames中划分shots，再再划分的shots中，选择key shots。优点在于，单个shots的长度和shots的个数可变不固定，问题是默认了一个滑动窗口中必然出现shot boundary，这个是不一定的。选confidence score是用softmax的形式（n取1）。选shot boundary和key shot用的是一样的方法。\n\t\n### 2. Analysis\n\n由于这篇文章需要label，标记正确key shots的位置来训练，所以还有待改进。说到底，还是没有真正找到frames（shots）在高维空间中的联系（loss function），所以只能有监督。","tags":["LSTM"],"categories":["Paper Reading"]},{"title":"Unsupervised Video Summarization via Relation-aware Assignment Learning","url":"/2021/04/19/Unsupervised Video Summarization via Relation-aware Assignment Learning/","content":"\n### 0. Overview\n\n![](/images/Unsupervised Video Summarization via Relation-aware Assignment Learning/2.png)\n\n优点：使用GNN来表达relation，unsupervised\n缺点：clip的划分，loss的选取，纯pixel提取\n兴趣程度：7\n\n### 1. Understanding\n\n将一个视频分成等长的n个clips，e.g. 10000帧的视频分成等长的20个clips，每个clip包含500帧。每个clip feature作为GNN中的一个node。node之间的edge关系，不是用邻接矩阵表示，而且用全连接的网络得到。node之间传递信息的时候，也是用全连接，把edge的信息当作先验，更新node。\nMLP是为了进一步提取feature，经过MLP后，得到n个更加representative的feature clips。用L2 norm得到它们的importance score（这边应该是通过node间的联系紧密程度来得到的），然后取前k个作为summary embedding其他为non-summary embedding。另外GNN中所有node合起来得到video embedding，这三个embedding一起放入loss。用了triplet loss，想拉近v和s，拉远v和n。\n\t\n### 2. Analysis\n\n可部分参考。","tags":["GNN","VideoSummarization"],"categories":["Paper Reading"]},{"title":"Learning on Attribute-Missing Graphs","url":"/2020/12/29/Learning-on-Attribute-Missing-Graphs/","content":"\n### 0. Introduction\n\n![](/images/Learning-on-Attribute-Missing-Graphs/structure.png)\n\nPaper link: https://arxiv.org/pdf/2011.01623.pdf\n\nCode link: https://github.com/xuChenSJTU/SAT-master-online\n\n主要分析一下算法部分，实验部分不涉及。是VAE+GAN应用于graph的一个文章。用了两个VAE，分别训练X(attribute)和A(structure)。文章假设X和A的$z$在高维空间中遵从同一分布，然后用GAN对他们的高维分布$z$进行加强。\n\n\n\n### 1. VAE\n\n普通的AE只要，求两个分布p和q使得：\n$$\n\\hat x \\approx q(z)\\\\\nz = p(x)\n$$\n但是实际上操作，有很多局限性。VAE对$z$做限制，使其遵从某个分布，这样如果直接sample一个这个分布给他作为$q(x)$的输入，就可以得到一个相对纯AE来说不错的结果。$p$和$q$是两个NN。\n\nVAE先推导一下。手上有一堆观测数据$x$，现在要求他们的分布$p(x)$。由于分布可能非常复杂，这里用GMM来表示。假设$x$由一个隐变量$z$控制。那么有\n$$\np(x)=\\int_z p(x|z)p(z)\n$$\n接下来就是要求$p(x)$的MLE。即maximize\n$$\nL=\\sum_x logp(x)\n$$\n做个恒等变形，\n$$\n\\begin{equation}\n\\begin{split}\nlogp(x)&=\\int_z q(z|x)log~p(x)dz=\\int_z q(z|x)log\\frac{p(x,z)}{p(z|x)}dz \\\\\n&=\\int_z q(z|x)log\\frac{p(x,z)}{q(z|x)}dz+\\int_z q(z|x)log\\frac{q(z|x)}{p(z|x)}dz \\\\\n&=\\int_z q(z|x)log\\frac{p(x,z)}{q(z|x)}dz +KL(q(z|x)||p(z|x))\\\\\n&\\ge \\int_z q(z|x)log\\frac{p(x,z)}{q(z|x)}dz=\\int_z q(z|x)log\\frac{p(x|z)p(z)}{q(z|x)}dz=L_b\n\\end{split}\n\\nonumber\n\\end{equation}\n$$\n因而，就是要maximize$L_b$，而\n$$\n\\begin{equation}\n\\begin{split}\nL_b&=\\int_z q(z|x)log\\frac{p(z)}{q(z|x)}dz+\\int_z q(z|x)logp(x|z)dz\\\\\n&=-KL(q(z|x)||p(z))+\\int_z q(z|x)logp(x|z)dz\n\\end{split}\n\\nonumber\n\\end{equation}\n$$\n即minimize前者，maximize后者。前者只需训练 encoder $q$ 让他输出的分布 $z$ 接近 $p(z)$ 即可。后者可写成\n$$\nE_{x\\sim q(z|x)}p(x|z)\n$$\n即普通AE的工作。\n\n### 2. SAT\n\nVAE+GAN的组合并不新鲜，但是这篇文章把它运用于重构图的节点信息，算是一个创新点。$X$表示的是每个node的attribute，$A$是整个图的structure。用两个VAE用于分别重构attribute和structure。其中，$E_x$、$D_x$、$D_A$是MLP，而$E_A$是GNN，其中的原因不是很懂。这篇文章是基于一个假设的，即：\n\n> In graph structured data, each node’s attributes and structures are correlated together and can be represented in a shared-latent space.\n\n因为如果不这样，就不能让两个VAE通过隐变量$z$互相学习了。不过这个假设背后的数学原理（是否有），目前还不懂，但这个假设是非常关键的一步。所以$z$其实是一个桥梁，充当了两个角色，1）$E_x->z->D_A$和$E_A->z->D_x$；2）$z_A$和$z_X$的分布用GAN去拟合。所以这个VAE+GAN和之前有篇VAE-GAN本质上不一样，之前的是用GAN去强化$x$和$\\hat x$，使单个VAE的表现更好，图像更清晰，而这篇文章是用GAN去使两个$z$遵从同一分布。\n\n最后贴一下损失函数作为总结，首先是VAE部分的，\n$$\n\\begin{equation}\n\\begin{split}\nmin_{\\theta_x,\\theta_a,\\phi_x,\\phi_a}\nL_r =&− E_{x_i\\sim p_X} [E_{q_{\\phi_x}(z_x|x_i)}[logp_{\\theta_x}(x_i|z_x)]]\\\\\n&− E_{a_i\\sim p_A} [E_{q_{\\phi_a}(z_a|a_i)}[log p_{\\theta_a}(a_i|z_a)]]\\\\\n&− λ_c · E_{a_i\\sim p_A} [E_{q_{\\phi_a}(z_a|a_i)}[log p_{\\theta_x}(x_i|z_a)]]\\\\\n&− λ_c · E_{x_i\\sim p_X} [E_{q_{\\phi_x}(z_x|x_i)}[log p_{\\theta_a}(a_i|z_x)]]\n\\end{split}\n\\nonumber\n\\end{equation}\n$$\n其中，$\\lambda_c$是调节上下两个的权重的。接下来是GAN的，\n$$\n\\begin{equation}\n\\begin{split}\nmin_\\psi max_{\\phi_x,\\phi_a} L_{adv} = &− E_{z_p\\sim p(z)}[log D(z_p)]\\\\\n&− E_{z_x\\sim q_{\\phi_x}(z_x|x_i)}[log(1 − D(z_x))]\\\\\n&− E_{z_p\\sim p(z)}[log D(z_p)]\\\\\n&− E_{z_a\\sim q_{\\phi_a}(z_a|a_i)}[log(1 − D(z_a))]\n\\end{split}\n\\nonumber\n\\end{equation}\n$$\n其中，$\\psi$是D的参数，$z_p$是label，来自某个分布，这里是Gaussian。最后是总的。\n$$\nmin_\\Theta max_\\Phi L = L_r + L_{adv}\n$$\n其中，$\\Theta = \\{\\theta_x, \\theta_a, \\phi_x, \\phi_a, \\psi\\}$ ，$ Φ = \\{\\phi_x, \\phi_a\\}$。\n\n","tags":["GNN"],"categories":["Paper Reading"]},{"title":"pip 镜像加速","url":"/2020/03/30/pip-镜像加速/","content":"\n### 1.临时指定源\n\n```bash\n# xxxxxxx 为安装对象\npip install -i https://pypi.tuna.tsinghua.edu.cn/simple xxxxxxx\n```\n\n</br>\n\n### 2.永久设定\n\n```bash\n # Linux   下创建或进入\n # ~/.pip/pip.conf \n [global]\n index-url = https://pypi.tuna.tsinghua.edu.cn/simple\n \n # Windows 下创建或进入\n # C:/User/xxxxxxx/pip/pip.ini\n [global]\n index-url = https://pypi.tuna.tsinghua.edu.cn/simple\n```\n\n","tags":["下载加速"],"categories":["Linux"]},{"title":"终端加速","url":"/2020/03/30/终端加速/","content":"\n```\n# Port number may vary \n\n# Linux (electron_ssr)\nexport http_proxy=http://127.0.0.1:12333\nexport https_proxy=http://127.0.0.1:12333\nexport http_proxy=socks5://127.0.0.1:12333\nexport https_proxy=socks5://127.0.0.1:12333\n\n# Windows (cmd)(git)\nset http_proxy=http://127.0.0.1:1080\nset https_proxy=http://127.0.0.1:1080\nset http_proxy=socks5://127.0.0.1:1080\nset https_proxy=socks5://127.0.0.1:1080\n\n```\n\n","tags":["下载加速"],"categories":["Linux"]},{"title":"LSF 作业调度系统","url":"/2020/03/24/LSF作业调度系统/","content":"\n### 0. Intro\n\n最近在搞 Vehicle Re-Id 的时候，要跑的模型太大，有幸用到了某单位的超算。用的是 LSF 作业调度系统。官方给的指南不太全，自己整理了几个常用的指令。\n\n### 1. OS & Software\n\nWindows 10 ( Linux 下没有 GUI，用 FTP 传文件还是不方便，用 windows 的话就是直接文件交互比较方便 )\n\n软件用的是 MobaXterm。\n\n\n### 2. Command List\n\n#### bsub < lsf.sh\n\n其中， lsf.sh 是自己写的一个脚本，之前犯错没输 <, 然后提交总是通不过。脚本范例如下：\n\n```\n# !/bin/sh\n# BSUB -q gpu_v100\n# BSUB -m \"gpu10\"\n# BSUB -gpu num=4:mode=exclusive_process\n# BSUB -o %J.out\n# BSUB -n 1\n# BSUB -e %J.err\n# BSUB -J Firsttry\npython3 /xxx/xxx/xxx/xxx.py\n```\n\n#### bjobs\n\n该指令可以查看当前正在运行的作业。\n\n![](/images/LSF作业调度系统/bjobs.jpg)\n\n\n#### bjobs -l JOB_ID\n\n该指令可以查看当前正在运行的作业的详细情况。\n\n![](/images/LSF作业调度系统/bjobs-l.jpg)\n\n\n\n#### bjobs -p JOB_ID\n\n该指令可以查看作业 PENDING 的原因。\n\n![](/images/LSF作业调度系统/bjobs-p.jpg)\n\n\n\n#### bkill JOB_ID\n\n该指令可以结束作业。\n\n![](/images/LSF作业调度系统/bkill.jpg)\n\n\n\n#### bkill -r JOB_ID\n\n由于刚接触 LSF，不太了解运行机制。自己写了一些非常烂的程序，提交上去，导致怎么 bkill 都结束不了作业，然后输入 info bkill 查看了一下它的详细用法。这个可以直接杀死进程。\n\n![](/images/LSF作业调度系统/bkill-r.jpg)\n\n\n\n#### bstop JOB_ID** 与 **bresume JOB_ID\n\n挂起和恢复作业，一般不用。如果作业不想继续了，最好直接 kill，否则占用资源。\n\n\n\n#### bpeek JOB_ID 与 bpeek -f JOB_ID\n\n该指令可以显示程序当前的屏幕输出。\n\n![](/images/LSF作业调度系统/bpeek.jpg)\n\n\n#### lsload 与 lsload -gpuload\n该指令可以输出负载。\n\n![](/images/LSF作业调度系统/lsload.jpg)\n\n\n\n#### bhosts 与 bqueues\n\n该指令可以查看各个节点和队列的作业信息。\n\n![](/images/LSF作业调度系统/bhosts.jpg)\n\n![](/images/LSF作业调度系统/bqueues.jpg)\n\n\n\n### 3. Ending\n\n最后祝自己以后还有机会用这样的 GPU 集群。:)","tags":["LSF"],"categories":["Thinking"]},{"title":"GAN","url":"/2020/02/17/GAN/","content":"\n### 0. Description\n\n这篇 post 将所有了解过的 GAN 都写下来, 方便回忆原理.\n\n\n\n### 1. GAN\n\n最初 GAN 的目的是想 generate 出和真实图片非常像的图片. 有个有个图片集 data, 假设它在某维空间满足某个分布: \n\n\n\n<div  align=\"center\">    \n <img src=\"/images/GAN/1.png\" width = \"50\" height = \"35\" align=center />\n</div>\n\n但是现在不知道这个分布的 formula, 现在可以做的是从这个 data 的分布中, sample 出一些图片. Generator 就希望能通过 network 学到一个分布:\n\n<div  align=\"center\">    \n <img src=\"/images/GAN/2.png\" width = \"33\" height = \"33\" align=center />\n</div>\n\n希望得到:\n\n\n<div  align=\"center\">    \n <img src=\"/images/GAN/3.png\" width = \"110\" height = \"38\" align=center />\n</div>\n\n也就是说, 当这两个分布相等时, 随便从 generator 学到的 distribution 中 sample 产生一个图片, 都会是在 data 的distribution 中的, 也就是说会是真实的图片.\n\n因为 generator 是个 network, 所以它要做的就是学到一个 network, 使得这两个分布的 divergence 最小:\n\n<div  align=\"center\">    \n <img src=\"/images/GAN/4.png\" width = \"370\" height = \"40\" align=center />\n</div>\n\n由于这两个分布它们的 formula 都不知道, 所以 divergence 其实是不能直接算的. 这个时候先把这个问题放一边, 看一下 discriminator.\n\n\nDiscriminator 要做的事情是区分真实的 image 和 generated 的 image. 如果是从 data distribution 中 sample 出来的, 就给高分; 如果是从 generated distribution 中 sample 出来的, 就给低分: \n\n<div  align=\"center\">    \n <img src=\"/images/GAN/7.png\" width = \"265\" height = \"45\" align=center />\n</div>\n\n所以可以得到以下 cost function, 希望 V 越大越好, 已经经过了一部分等价转换:\n\n<div  align=\"center\">    \n <img src=\"/images/GAN/5.png\" width = \"570\" height = \"40\" align=center />\n</div>\n\n把期望转成积分的形式:\n\n<div  align=\"center\">    \n <img src=\"/images/GAN/6.png\" width = \"550\" height = \"42\" align=center />\n</div>\n\n预期是, 无论输入什么样的 x, 只要来自于 generator 就给低分, 只要来自于 real sample 就给高分, 所以其实 D of x 的在某处(e.g. x = 0)的值不受任何附近(e.g. (-1, 1))的值的影响, 完全可以看成是一元函数求极值. \n\n<div  align=\"center\">    \n <img src=\"/images/GAN/8.png\" width = \"270\" height = \"53\" align=center />\n</div>\n\n最后算出来, 理论上 D 取这个值的时候 V 可以达到最大, 也就是说 discriminator 的性能最好. 但是两个分布都不知道, 所以没有解析解, 真正操作时这个用不了. 但是把上式代入 V 中, 整理后可得:\n\n<div  align=\"center\">    \n <img src=\"/images/GAN/9.png\" width = \"600\" height = \"50\" align=center />\n</div>\n\n发现这个正好是 JS divergence 的形式. 又由于最初要求的就是 g 和 data distribution 的 divergence, 所以把上式代入 G star 得:\n\n<div  align=\"center\">    \n <img src=\"/images/GAN/10.png\" width = \"320\" height = \"52\" align=center />\n</div>\n\n这个式子就可以用 network 求了. 先固定 Generator 训练 Discriminator 让 V 达到 max, 再固定 Discriminator  训练 Generator 使得 V 取最小值. Iteratively 训练.\n\n\n\n```\nimport argparse\nimport os\nimport numpy as np\nimport math\n\nimport torchvision.transforms as transforms\nfrom torchvision.utils import save_image\n\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torch.autograd import Variable\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\n\nos.makedirs(\"images\", exist_ok=True)\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--n_epochs\", type=int, default=200, help=\"number of epochs of training\")\nparser.add_argument(\"--batch_size\", type=int, default=64, help=\"size of the batches\")\nparser.add_argument(\"--lr\", type=float, default=0.0002, help=\"adam: learning rate\")\nparser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: decay of first order momentum of gradient\")\nparser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\nparser.add_argument(\"--n_cpu\", type=int, default=8, help=\"number of cpu threads to use during batch generation\")\nparser.add_argument(\"--latent_dim\", type=int, default=100, help=\"dimensionality of the latent space\")\nparser.add_argument(\"--img_size\", type=int, default=28, help=\"size of each image dimension\")\nparser.add_argument(\"--channels\", type=int, default=1, help=\"number of image channels\")\nparser.add_argument(\"--sample_interval\", type=int, default=400, help=\"interval betwen image samples\")\nopt = parser.parse_args()\nprint(opt)\n\nimg_shape = (opt.channels, opt.img_size, opt.img_size)\n\ncuda = True if torch.cuda.is_available() else False\n\n\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n\n        def block(in_feat, out_feat, normalize=True):\n            layers = [nn.Linear(in_feat, out_feat)]\n            if normalize:\n                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *block(opt.latent_dim, 128, normalize=False),\n            *block(128, 256),\n            *block(256, 512),\n            *block(512, 1024),\n            nn.Linear(1024, int(np.prod(img_shape))),\n            nn.Tanh()\n        )\n\n    def forward(self, z):\n        img = self.model(z)\n        img = img.view(img.size(0), *img_shape)\n        return img\n\n\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(int(np.prod(img_shape)), 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(256, 1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, img):\n        img_flat = img.view(img.size(0), -1)\n        validity = self.model(img_flat)\n\n        return validity\n\n\n# Loss function\nadversarial_loss = torch.nn.BCELoss()\n\n# Initialize generator and discriminator\ngenerator = Generator()\ndiscriminator = Discriminator()\n\nif cuda:\n    generator.cuda()\n    discriminator.cuda()\n    adversarial_loss.cuda()\n\n# Configure data loader\nos.makedirs(\"../../data/mnist\", exist_ok=True)\ndataloader = torch.utils.data.DataLoader(\n    datasets.MNIST(\n        \"../../data/mnist\",\n        train=True,\n        download=True,\n        transform=transforms.Compose(\n            [transforms.Resize(opt.img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n        ),\n    ),\n    batch_size=opt.batch_size,\n    shuffle=True,\n)\n\n# Optimizers\noptimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\noptimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n\nTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n\n# ----------\n#  Training\n# ----------\n\nfor epoch in range(opt.n_epochs):\n    for i, (imgs, _) in enumerate(dataloader):\n\n        # Adversarial ground truths\n        valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), requires_grad=False)\n        fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0), requires_grad=False)\n\n        # Configure input\n        real_imgs = Variable(imgs.type(Tensor))\n\n        # -----------------\n        #  Train Generator\n        # -----------------\n\n        optimizer_G.zero_grad()\n\n        # Sample noise as generator input\n        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], opt.latent_dim))))\n\n        # Generate a batch of images\n        gen_imgs = generator(z)\n\n        # Loss measures generator's ability to fool the discriminator\n        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n\n        g_loss.backward()\n        optimizer_G.step()\n\n        # ---------------------\n        #  Train Discriminator\n        # ---------------------\n\n        optimizer_D.zero_grad()\n\n        # Measure discriminator's ability to classify real from generated samples\n        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n        d_loss = (real_loss + fake_loss) / 2\n\n        d_loss.backward()\n        optimizer_D.step()\n\n        print(\n            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n            % (epoch, opt.n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())\n        )\n\n        batches_done = epoch * len(dataloader) + i\n        if batches_done % opt.sample_interval == 0:\n            save_image(gen_imgs.data[:25], \"images/%d.png\" % batches_done, nrow=5, normalize=True)\n\n```\n\n","categories":["Theoretical Analysis"]},{"title":"sorted 对 class 元素排序","url":"/2020/02/12/sorted函数-对list-class排序/","content":"\n### Example\n\n```python\nclass unit(object):\n    def __init__(self, value, label):\n        self.value = value\n        self.label = label\n        \n        \na = [unit(None, None) for i in range(1000)]\n\n# 对a赋值后\n# reverse = True 降序, 默认为 False 升序\na = sorted(a, key=lambda student: student.value, reverse = True)\n```\n\n### student 可随便换其他名字","categories":["Practical Codes"]},{"title":"mAP CMC","url":"/2020/02/12/mAP-CMC/","content":"\n## mAP**(Mean Average Precision)**\n\n### 0.\tTP & FN & TN & FP\n\n#### TP 是 true positive, 表示正样本被判断为正样本(即T, true). FN 是 false negative, 表示 negative sample 被判断为 positive sample (即F, false). 另外两个也一样.\n\n### 1.\tPrecision & Recall\n\n#### 分类问题中, 现在已有每个样本的概率值和标签 (0, 1). 将样本按照概率从高到底排序. 分类器表现理想的情况是, 标签为 1 的样本都在上面, 标签为 0 的样本都在下面. 有个 threshold, 一开始指向最上面的 sample, 然后每次下移一个, threshold 及以上的都被认为是 positive sample, 下面的都被认为是 negative sample.\n\n#### Precision = TP / (TP + FN)\t假设 TP + FN = k, precision 就表示前 k 个样本中, \"negative sample 的比例\", 因为它们本来应该呆在 positive sample 下面的, 但它们现在跑到 positive sample 上面去了. 有几个 positive sample 就计算几个 precision, threshold 每次下移一个, 当移到某个 sample 为 positive sample 的时候再算 precision.\n\n#### Recall = TP / (TP + FP) \t表示 positive sample 被成功找回来的概率. 因为 positive sample 总数不变, 所以 TP + FP 不变. Recall 最终会为1.\n\n### 2.\tAP & mAP\n\n#### 假设有\n\n$$\nAP =\\frac{\\sum^N_1 precision}{N}\n$$\n\n</br>\n\n## CMC**(Cumulative Match Characteristics)**\n\n### \n\n</br>","categories":["Metrics"]},{"title":"Image.convert转化图片","url":"/2019/12/20/Image-convert转化图片/","content":"\n```reStructuredText\n 1 ------------------（1位像素，黑白，每字节一个像素存储）\n L ------------------（8位像素，黑白）\n P ------------------（8位像素，使用调色板映射到任何其他模式）\n RGB-------------- （3x8位像素，真彩色）\n RGBA-------------（4x8位像素，带透明度掩模的真彩色）\n CMYK-------------（4x8位像素，分色）\n YCbCr------------ （3x8位像素，彩色视频格式）\n I--------------------（32位有符号整数像素）\n F------------------- （32位浮点像素）\n```\n\n\n\n```python\nimg = Image.open(path).convert('1')\n```\n\n\n\n<div  align=\"center\">    \n <img src=\"/images/Image.convert转化图片/a.png\" width = \"320\" height = \"240\" align=center />\n</div>\n\n\n\n```python\nimg = Image.open(path).convert('L')\n```\n\n<div  align=\"center\">    \n <img src=\"/images/Image.convert转化图片/b.png\" width = \"320\" height = \"240\" align=center />\n</div>\n\n```python\nimg = Image.open(path).convert('P')\n```\n\n<div  align=\"center\">    \n <img src=\"/images/Image.convert转化图片/c.png\" width = \"320\" height = \"240\" align=center />\n</div>\n\n```\nimg = Image.open(path).convert('RGB')\n```\n\n<div  align=\"center\">    \n <img src=\"/images/Image.convert转化图片/d.png\" width = \"320\" height = \"240\" align=center />\n</div>\n\n```\nimg = Image.open(path).convert('RGBA')\n```\n\n<div  align=\"center\">    \n <img src=\"/images/Image.convert转化图片/e.png\" width = \"320\" height = \"240\" align=center />\n</div>\n\n```python\nimg = Image.open(path).convert('CMYK')\n```\n\n<div  align=\"center\">    \n <img src=\"/images/Image.convert转化图片/f.png\" width = \"320\" height = \"240\" align=center />\n</div>\n\n```python\nimg = Image.open(path).convert('YCbCr')\n```\n\n<div  align=\"center\">    \n <img src=\"/images/Image.convert转化图片/g.png\" width = \"320\" height = \"240\" align=center />\n</div>\n\n```python\nimg = Image.open(path).convert('I')\n```\n\n<div  align=\"center\">    \n <img src=\"/images/Image.convert转化图片/h.png\" width = \"320\" height = \"240\" align=center />\n</div>\n\n```python\nimg = Image.open(path).convert('F')\n```\n\n<div  align=\"center\">    \n <img src=\"/images/Image.convert转化图片/k.png\" width = \"320\" height = \"240\" align=center />\n</div>","categories":["Practical Codes"]},{"title":"模型的保存与载入","url":"/2019/12/19/模型的保存与载入/","content":"\n### 跑大网络一次跑不完，就要保存了下次再跑。有两种保存方式，一种是保存网络基本结构(e.g. 每层的W, b)，一种是整个网络所有东西全部保存。网上推荐使用第一种，因为如果网络过大，用第二种方法加载网络会比较慢，而且占地方也大。我只试过第一种，Pytorch 例子如下。\n\n</br>\n\n### 保存：\n\n```python\ntorch.save(net.state_dict(), model_path + '/model-inter-' + str(batch_id+1) + \".pt\")\n```\n\n</br>\n\n### 加载， map_location 要写清楚用的是 CPU 还是 GPU，用的是哪几块，不然可能会报错：\n\n```python\nnet = Siamese()\nnet.load_state_dict(torch.load('/home/yk/siamese-pytorch/models/model-inter-104001.pt', map_location='cuda:0')\n```\n\n","tags":["Pytorch"],"categories":["Practical Codes"]},{"title":"制作自己的 dataset","url":"/2019/12/01/制作自己的dataset/","content":"\n### 制作自己的 dataset 要写一个自己的 Dataset 类，然后用 Dataloader  封装就好了。\n\n---\n\n\n\n\n\n# 一、Dataset 制作\n\n### 一般 Dataset 类有以下的函数：\n\n\n\n```python\nclass Dataset(Dataset):\n\n    def __init__(self, ...):\n\t\t...\n    def __getitem__(self, index):\n\t\t...\n    def __len__(self):\n\t\t...\n\t\n```\n\n</br>\n\n### \\_\\_init\\_\\_() 是初始化时传入的一些参数，不是非常重要。\n\n### \\_\\_getitem\\_\\_() 需要自己修改，功能是将图片从磁盘中读入，存入 array 或者 list 中。\n\n### \\_\\_len()\\_\\_ 是返回 Dataset 的大小，需要仔细设计。\n\n### 下面是两个用于构建Siamese Nerual Network 的 example 。\n\n### 这个是事先将图片的 path 都写入了一个 txt 文件中，然后 getitem 就从 txt 中读图片路径。\n\n```python\nclass MyDataset(Dataset):\n\n    def __init__(self, txt, transform=None, target_transform=None, should_invert=False):\n\n        self.transform = transform\n        self.target_transform = target_transform\n        self.should_invert = should_invert\n        self.txt = txt\n\n    def __getitem__(self, index):\n\n        line = linecache.getline(self.txt, random.randint(1, self.__len__()))\n        line.strip('\\n')\n        img0_list = line.split()\n        # 判断是否要选同一个类别的图片\n        should_get_same_class = random.randint(0, 1)\n        if should_get_same_class:\n            while True:\n                # 随机在 txt 中选一行读取，每一行有一张图片的 path 和它的 label\n                img1_list = linecache.getline(self.txt, random.randint(1, self.__len__())).strip('\\n').split()\n                if img0_list[1] == img1_list[1]:\n                    break\n        else:\n            img1_list = linecache.getline(self.txt, random.randint(1, self.__len__())).strip('\\n').split()\n\n        img0 = Image.open(img0_list[0])\n        img1 = Image.open(img1_list[0])\n        # 转图片通道数\n        img0 = img0.convert(\"L\")\n        img1 = img1.convert(\"L\")\n\n        # 非必要的\n        if self.should_invert:\n            img0 = PIL.ImageOps.invert(img0)\n            img1 = PIL.ImageOps.invert(img1)\n\n        # 非必要的\n        if self.transform is not None:\n            img0 = self.transform(img0)\n            img1 = self.transform(img1)\n\n        return img0, img1, torch.from_numpy(np.array([int(img1_list[1] != img0_list[1])], dtype=np.float32))\n\n    # 这个 len 的返回长度就是 txt 中图片总数\n    def __len__(self):\n        fh = open(self.txt, 'r')\n        num = len(fh.readlines())\n        fh.close()\n        return num\n```\n\n</br>\n\n### 这个是事先没有准备 txt 目录，所以写了一个 loadToMem 去每个文件夹里面找图片。\n\n```python\nclass OmniglotTest(Dataset):\n\n    def __init__(self, dataPath, transform=None, times=100, way=166):\n        np.random.seed(1)\n        super(OmniglotTest, self).__init__()\n        self.transform = transform\n        self.times = times\n        self.way = way\n        self.img1 = None\n        self.c1 = None\n        self.datas, self.num_classes = self.loadToMem(dataPath)\n\n    def loadToMem(self, dataPath):\n        print(\"begin loading test dataset to memory\")\n        # datas 是读取到的图片的总 list，idx 是类别数。\n        datas = {}\n        idx = 0\n        for alphaPath in os.listdir(dataPath):\n            # for charPath in os.listdir(os.path.join(dataPath, alphaPath)):\n                datas[idx] = []\n                for samplePath in os.listdir(os.path.join(dataPath, alphaPath)):\n                    filePath = os.path.join(dataPath, alphaPath, samplePath)\n                    s = Image.open(filePath).convert('L')\n                    # 调整图片尺寸\n                    s = s.resize((105, 105), Image.ANTIALIAS)\n                    datas[idx].append(s)\n                idx += 1\n        print(\"finish loading test dataset to memory\")\n        return datas, idx\n\n    def __len__(self):\n        return self.times * self.way\n\n    def __getitem__(self, index):\n        idx = index % self.way\n        label = None\n        # generate image pair from same class\n        if idx == 0:\n            self.c1 = random.randint(0, self.num_classes - 1)\n            self.img1 = random.choice(self.datas[self.c1])\n            img2 = random.choice(self.datas[self.c1])\n        # generate image pair from different class\n        else:\n            c2 = random.randint(0, self.num_classes - 1)\n            while self.c1 == c2:\n                c2 = random.randint(0, self.num_classes - 1)\n            img2 = random.choice(self.datas[c2])\n\n        if self.transform:\n            img1 = self.transform(self.img1)\n            img2 = self.transform(img2)\n        # print('datas.shape = ', len(self.datas))\n        # print('img1 = ', img1.shape)\n        # print('img2 = ', img2.shape)\n        return img1, img2\n\n```\n\n</br>\n\n---\n\n\n\n# 二、Dataloader 封装\n\n### 这个比较简单，因为 Dataloader 是 Pytorch 内置的函数。\n\n```python\ntrain_dataloader = DataLoader(dataset=train_data, shuffle=True, num_workers=2, batch_size=Config.train_batch_size)\n```\n\n</br>\n\n### 主要需要关注的是 batch_size 。这个以及之前提到的 len 和训练的次数有关。\n\n```python\nfor i, data in enumerate(train_dataloader, 0):\n```\n\n</br>\n\n### 其中，for 循环的次数 = len / batch_size 。\n\n\n\n","categories":["Practical Codes"]},{"title":"RuntimeError: size mismatch, m1: [128 x 184320], m2: [9216 x 4096]","url":"/2019/11/30/size-mismatch/","content":"\n### 今天跑代码遇到这个问题，发现出错的位置是在卷积层到全连接层的地方。卷积网络的每层节点个数都是事先定好的，这对input的像素也有要求，必须也是和预先设定的像素相同，否则卷积层最后得出的结果会与设计的不同。\n\n### 卷积层结果一般是 [batchsize, dimension0, 1, 1]， 全连接层只有二维 [batchsize, dimensio_0]，会自动对之前四维的卷积结果进行压缩。如果最后卷积结果不是 [batchsize, dimension_0, 1, 1] 的形式，就不会被压缩成 [batchsize, dimension_0] 的形式，而是别的形式。下面就算不下去了。\n\n</br>\n\n![](/images/size-mismatch/1.png)\n\n</br>\n\n### 像下图这个网络参数对应的输入图像尺寸应该是 105 x 105 的，但是我在预处理的时候没考虑到这点，虽然事后可以debug出来，但是应该事先要考虑到这点。\n\n</br>\n\n![](/images/size-mismatch/3.png)","tags":["Python错误"],"categories":["Thinking"]},{"title":"将dataset标签写入txt","url":"/2019/11/26/将dataset标签写入txt/","content":"\n### 从网上获取的数据集，想做成自己想要的样子，要为该数据集做一个 label.txt 方便以后的训练。比如像这样的，第一列是图片位置，第二列是标签：\n\n</br>\n\n![](/images/将dataset标签写入txt/1.png)\n\n</br>\n\n### 因为我原先的这个数据集里面每类样本数不一样，所以稍微要烦一丢丢，直接上代码。\n\n</br>\n\n```python\nimport os\n \nroots = \"/home/yk/下载/VERI-Wild/images/\"\t\t\t\t\n\n\ndef convert(train=True):\n    if train:\n        f = open(roots + 'train.txt', 'w')\t\t\t# 新建了一个train.txt存label的信息\n        data_path = roots + 'images/'\t\t\t\n        if not os.path.exists(data_path):\n            os.makedirs(data_path)\n        label = 0\n        for root, dirs, files in os.walk(data_path):\n            if len(dirs):\n                # print(dirs)\n                continue\n            else:\n                label = label + 1\n                s = len(files)\n                for i in range(s):\n                    img_path = root + '/' + files[i]\n                    # print(img_path + ' ' + str(label))\n                    f.write(img_path + ' ' + str(label) + '\\n')\t\t\t# 写入文件\n        f.close()\n\n\nconvert(train=True)\n\n```\n\n","categories":["Practical Codes"]},{"title":"删除错误的更新源","url":"/2019/11/26/删除错误的更新源/","content":"\n### Linux在执行 apt-get update时，有的时候会出现这样的情况。\n\n### </br>\n\n![](/images/删除错误的更新源/1.png)\n\n</br>\n\n### 之后每次对新的软件更新都会看到之前的错误提示，很烦。所以要把这些一直更新错误的软件信息删去。如下图操作，进入/etc/apt/sources.list.d，输入ls查看，可以看到上图中更新错误的hzwhuang-ubuntu-ss-qt5。\n\n</br>\n\n![](/images/删除错误的更新源/2.png)\n\n</br>\n\n### 输入 sudo rm XXXXX(文件名)，就可以删除该文件，把相关的list、save文件一并删除。\n\n</br>\n\n![](/images/删除错误的更新源/3.png)\n\n</br>\n\n### 这时再 apt-get update 时，刚刚那个错误就消失了。就这样，把所有的之前更新不了的软件信息都删掉，心旷神怡！！！\n\n</br>\n\n![](/images/删除错误的更新源/6.png)\n\n","categories":["Linux"]},{"title":"使用Github+Hexo搭建Blog","url":"/2019/11/24/first-time-record/","content":"\n### Happy.\n\n\n\n![](/images/c.jpg)\n\n\n\n","tags":["Life"],"categories":["Thinking"]}]